%!TeX root=main.tex
%Background, Motivation and Relevance – literature review
 %•	Outline and critically discuss what relevant research has been undertaken in the past in your area of interest and why this project is necessary. 
%•	Outline and critically discuss why the research will be of value in the future and what the anticipated impact of the outcomes will be.
%•	Using supporting literature, set out and justify where your work will fit in the body of computing knowledge. Provide a diagram such as a mind map illustrating your position in the computing body of knowledge.
%\•	Outline why you want to undertake this work. Some areas to consider are personal motivation, skills set and career choices, a gap in the knowledge base and or relevance to your programme of study.

There has been multiple studies looking at website attacks however typically these have focused on single variable analyses for example CPU depletion. Research in the area of high rate attacks don't pose the threat they used due to high volume of research in this area, meaning that preventative measures have been made. As Agrawal \& Tapaswi state "due to this huge volume of malicious traffic, such attacks can be easily detected. Thus, attackers are getting attracted towards the low-rate DDoS attacks, slowly. Low-rate DDoS attacks are difficult to detect due to their stealthy and low-rate traffic" (\cite{8794618}). Futhermmore in a study by \citeauthor{9016229} in 2020, they state that "Low-rate Denial of service (LDoS) attacks has become one of the biggest threats to the Internet, cloud computing platforms, and big data centers" (\cite{9016229}) showing the need for an effective attack detection tool.

Cloudflare statistics indicate that high rate ddos attacks are increasing yearly however due to the protocols in place high rate ddos attacks aren't successful with there effects being easily mitigated (\cite{Q3attacks}). Whereas low rate attacks can be more easily disguised alongside more genuine use of the website thereby bypassing the protocols in place for high rate attacks. 

The rate of request is not necessarily the only signal of an attack. If for example if an IP address is constantly searching for login pages or back up files this could be an indication of suspicious behaviour. There is not a lot of data on low rate attacks and this could be a cause of concern and if we don't know how many attacks are happening it may indicate there is no detection method for the attacks. 

 %Tripathi researched attacks using website data and by recording traffic over a 14 hour period and proposed an anomaly detection technique that attempted to detect attacks by measuring the Chi squared (X\textsuperscript{\small2}) differential value between the expected traffic pattern, the result suggested attacks could be found with high accuracy (\cite{tripathi2018slow}). This study suggested that attacks could be detected with high levels of accuracy. This study utilised simulated data and therefore whilst theoretically sound  has limited application in the real word. 
 
 Tripathi proposed an anomaly detection technique that attempted to detect attacks by measuring the Chi squared (X\textsuperscript{\small2}) differential value between the expected traffic pattern, the result suggested attacks could be found with high accuracy. Tripathi Collected 14 hours of HTTP/2 traffic, a fundamental issue with this being that the researcher simulated the data,(\cite{tripathi2018slow}) whilst theoretically sound there are no real world examples showing this to be effective. Therefore the data is not reliable because of the small time frame in which it was carried out and you cant say for sure whether the changes in traffic would've occurred naturally making it hard to differentiate peaks in traffic or an attack. Therefore this work looks to use real data over increase time-window to look to at its efficiency. This research looks to access large amounts of data and utilise data sciences techniques to organise it This will help move the field away from using simulated and unreliable data, to work with more real world data to improve the wider research in the field. 

Staniford, Hoagland and McAlerney suggest that storing large amounts of network traffic may be impractical \cite{staniford2002practical}. However \citeauthor{9016229} states that "A huge amount of network traffic can be collected, stored, organized and classified by big data analysis. Moreover, the detection judgement and defense decision can be achieved by analyzing unknown patterns" \cite{9016229} Therefore if there is a need for a large amount of data, that may be impractical to store. One solution may be to look at the data already available to analyse and designing a suitable way to analyse that data. Most website keep log files of who is accessing the website and activity history. Therefore theoretically the log files negate the need to collect extra data and may provide sufficient information to detect attacks. This method was tried by Smith P. However this only used a small data sample, but did have promising results as it was able to distinguish good traffic from bad traffic. 

Erwin Adi has done a lot of research into Low-rate Denial of Service (LDoS) attacks. His primary paper looks at CPU depletion as an indicator of attack. In the same paper, Adi himself admits that this maybe a flawed technique for attack detection. \cite{Adi2016} Most previous studies into detecting Low Bandwidth attacks only look at a single data point such as CPU. This research proposes to look across multiple data points to detect attacks.

All the research done to date looks at traffic flow in various ways, however it fails to take into account where that traffic is coming from, for example, most cyber attacks emanate from Russia and China, so the research is ignoring a key area that needs to be explored.


The work done by Smith proposed a formula that took many factors  into account. The overall formula was defined as  \[risk = (orrcancesOfipLog \times 0.6) + ((requestRisk+responseRisk) \times 0.3) + (countryRisk \times  0.1) \] this formula was the first to look at multiple data points when detecting low rate attacks however in Smith's conclusions they state that the network that the IP address comes from could potentially have a greater impact on the risk as due to VPN technology the country can change. Furthermore, the risk of a particular country only looks at the total number of attacks and Smith points out that  "the values used in the software are only based on the number of attacks per country" (\cite{smith}) therefore the overall calculation will be changed and the underlying risk assigned to each country will be assessed looking at attacks per head of population. Also, as well as the country, it would be useful to look at the network the IP comes from and build that into the calculation. When looking at the risk of an individual country, the values used in the software are only based on the number of attacks per country. While this is a good way to assess the risk of a country, this methodology could potentially have issues, for example, larger countries will statistically have more attacks than smaller countries. According to the Nexusguard 2018 2 threat report, it was identified that 23.34\% of attacks came from China, with a further 14.90\% coming from the USA; this is said to be expected, due to the internet presence of their citizens parentage per population, with China's being over one billion users. Therefore, it would be good to look at the total number of attacks per country that are reported to the software for instance, compared to the size of the population. 


\begin{wrapfigure}{L}{0.5\textwidth}
\label{web using h2}
    \centering
    \includegraphics[width=88mm,scale=0.8]{images/CF q3.png} 
    \caption{Attacks by country according to Cloudflare}
\end{wrapfigure}

According to Cloudflare in their DDoS threat report 2022, application layer DDoS attacks are mainly comming from China (\cite{Q3attacks})

%Tripathi put forward an approach in 2018 which as he suggests could accurately detect attacks. He came to this conclusion by using a sample of websites, and using the Chi squared method to detect low attacks by monitoring benchmarking. He concluded that using Chi squared differential it would be possible to detect an anomaly in traffic. The study took a sample of websites and attempted to detect low rate attacks by monitoring benchmarking and measuring the Chi squared (X\textsuperscript{\small2}) differential value between the expected and observed traffic pattern. Tripathi suggests this approach could detect attacks with high accuracy and may lead to future research to assess further HTTP/2 vulnerabilities, thus potentially mitigating these threat vectors with fixes. Tripathi indicated that although his detection method for attack traffic was successful; if HTTP/2 traffic data is encrypted it must then be decrypted before submitting traces to the detector. He suggested that this could be easily achieved with the aid of an intercepting proxy before forwarding to the target website responsible for handling HTTP/2 requests. It must be noted that most large companies are utilising this strategy to intercept traffic coming into their local network. (\cite{tripathi2018slow}). 

%Adi's 2017 work looked at some of the stealthier approaches that cyber attacks were using in order to bypass current detection methods. Adi and his team set up two models intended to simulate stealthy low rate DoS attacks which they called 'bots'. The investigation aimed to model attacks whose traffic continually consumed the victims computing resource, while still being stealthy enough to yield some false alarms via the target servers' 'learning' mechanisms. Adi constructed the attack 'bots' with four core factors for experimentation, these were number of threads, number of window\_update, stealthy factors and the delay between successive TCP connections. These two sets of Stealth models were tested against regular flash crowd traffic in an effort to differentiate the pattern. The experiment and subsequent analysis was successful in distinguishing a notable difference in the patterns of the number of packets carrying SYN flags per 1-second traffic instance between regular flashcrowd traffic and the simulated stealthy attacks (\cite{adi2017stealthy}). Since Adi's work there appears to be an implementation of the methodology proposed in their paper. For example, Cloudflare have been able to implement a defence mechanism against a SYN flood attack. The have created a program called 'gatebot' which monitors SYN packet requests and attempts to drop malicious SYN packets on the firewall layer (\cite{CFSYN}) the work does not refer to that of Adi's however the Cloudflare implementation does appear to use similar concepts. At the  time of writing this seems to be the only documented attempt of blocking SYN attacks.


\subsection{revlantcy}
The work is relevant due to the increasing number of websites increasing as nearly all businesses have a website. In the aftermath of remote working and lock downs E-commerce sites increased (CITE NEEDED) the types of attacks the study aims to detect can be hard to identify. Cloudflare notes in August  2022 that 

%attacks up on websites

As websites become part of daily life the number of sites online is increasing as such there are more and more potential targets for attackers and more exploits are discovered, as attacks get more complex the is a need for different detection techniques.



%sim normal traffic
% then sim attack