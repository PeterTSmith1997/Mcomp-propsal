%!TeX root=main.tex
%Background, Motivation and Relevance – literature review
 %•	Outline and critically discuss what relevant research has been undertaken in the past in your area of interest and why this project is necessary. 
%•	Outline and critically discuss why the research will be of value in the future and what the anticipated impact of the outcomes will be.
%•	Using supporting literature, set out and justify where your work will fit in the body of computing knowledge. Provide a diagram such as a mind map illustrating your position in the computing body of knowledge.
%\•	Outline why you want to undertake this work. Some areas to consider are personal motivation, skills set and career choices, a gap in the knowledge base and or relevance to your programme of study.


In a study by \citeauthor{9016229} in 2020, they state that "Low-rate Denial of service (LDoS) attacks has become one of the biggest threats to the Internet, cloud computing platforms, and big data centers" (\cite{9016229}) showing the need for an effective attack detection tool. Studies by Adi (summary here) however \citeauthor{9016229} notes that "Moreover, the detection judgement and defence decision can be achieved by analysing unknown patterns and correlations in network traffic, and grasping other relevant intrinsic information in network traffic."  (\cite{9016229}) this study aims to do that.


The method proposed in the aims, has not been documented as a approach to detect attacks, therefore there is a lack of literature so the review will focuses on gaps in knowledge and where this work fits within that.


Erwin Adi has done a lot of research into Low-rate Denial of Service (LDoS) attacks. His primary paper looks at CPU depletion as an indicator of attack. In the same paper, Adi himself admits that this maybe a flawed technique for attack detection. \cite{Adi2016} 

Most previous studies into detecting Low Bandwidth attacks only look at a single data point such as CPU. This research proposes to look across multiple data points to detect attacks. Furthermore Staniford, Hoagland and McAlerney suggest that storing large amounts of network traffic may be impractical \cite{staniford2002practical}. However \citeauthor{9016229} states that "A huge amount of network traffic can be collected, stored, organized and classified by big data analysis. Moreover, the detection judgement and defense decision can be achieved by analyzing unknown patterns" \cite{9016229} Therefore if there is a need for a large amount of data, that may be impractical to store. One solution may be to look at the data already available to analyse and designing a suitable way to analyse that data.

All the research done to date looks at traffic flow in various ways, however it fails to take into account where that traffic is coming from, for example, most cyber attacks emanate from Russia and China, so the research is ignoring a key area that needs to be explored.

%Tripathi put forward an approach in 2018 which as he suggests could accurately detect attacks. He came to this conclusion by using a sample of websites, and using the Chi squared method to detect low attacks by monitoring benchmarking. 
%He concluded that using Chi squared differential 
%study took a sample of websites and attempted to detect low rate attacks by monitoring benchmarking and measuring the Chi squared (X\textsuperscript{\small2}) differential value between the expected and observed traffic pattern. Tripathi suggests this approach could detect attacks with high accuracy and may lead to future research to assess further HTTP/2 vulnerabilities, thus potentially mitigating these threat vectors with fixes. Tripathi indicated that although his detection method for attack traffic was successful; if HTTP/2 traffic data is encrypted it must then be decrypted before submitting traces to the detector. He suggested that this could be easily achieved with the aid of an intercepting proxy before forwarding to the target website responsible for handling HTTP/2 requests. It must be noted that most large companies are utilising this strategy to intercept traffic coming into their local network. (\cite{tripathi2018slow}). 

%Adi's 2017 work looked at some of the stealthier approaches that cyber attacks were using in order to bypass current detection methods. Adi and his team set up two models intended to simulate stealthy low rate DoS attacks which they called 'bots'. The investigation aimed to model attacks whose traffic continually consumed the victims computing resource, while still being stealthy enough to yield some false alarms via the target servers' 'learning' mechanisms. Adi constructed the attack 'bots' with four core factors for experimentation, these were number of threads, number of window\_update, stealthy factors and the delay between successive TCP connections. These two sets of Stealth models were tested against regular flash crowd traffic in an effort to differentiate the pattern. The experiment and subsequent analysis was successful in distinguishing a notable difference in the patterns of the number of packets carrying SYN flags per 1-second traffic instance between regular flashcrowd traffic and the simulated stealthy attacks (\cite{adi2017stealthy}). Since Adi's work there appears to be an implementation of the methodology proposed in their paper. For example, Cloudflare have been able to implement a defence mechanism against a SYN flood attack. The have created a program called 'gatebot' which monitors SYN packet requests and attempts to drop malicious SYN packets on the firewall layer (\cite{CFSYN}) the work does not refer to that of Adi's however the Cloudflare implementation does appear to use similar concepts. At the  time of writing this seems to be the only documented attempt of blocking SYN attacks.


\subsection{revlantcy}
The work is relevant due to the increasing number of websites increasing as nearly all businesses have a website. In the aftermath of remote working and lock downs E-commerce sites increased (CITE NEEDED) the types of attacks the study aims to detect can be hard to identify. Cloudflare notes in August  2022 that 

%attacks up on websites

As websites become part of daily life the number of sites online is increasing as such there are more and more potential targets for attackers and more exploits are discovered, as attacks get more complex the is a need for different detection techniques.










%Tripathi proposed an anomaly detection technique that proposed the attacks work by measuring the Chi squared (X\textsuperscript{\small2}) differential value between the expected traffic pattern, as a result showing that it could detect the attacks with high accuracy. He Collected 14 hours of HTTP/2. An issue with this being that he simulated the data,(put reference)  by simulating normal website traffic for 14 hours and then doing the same but with attack traffic for 14 hours. Therefore the data is not reliable because of the small time frame in which it was carried out and you cant say for sure whether the changes in traffic would've occurred had the data not been simulated.  Further since he already knew what he was looking for he could've changed the method to fit the conclusion. Therefore this work will use real data to prove its work.This will help move the field away from using simulated and unreliable data, to work with more unsimulated data to improve the wider research in the field. 

%sim normal traffic
% then sim attack